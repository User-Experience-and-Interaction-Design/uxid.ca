<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <title>Publications</title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="description" content="Research Lab, Home, Ontario Tech University">
      <meta name="author" content="">
      <!-- Le styles -->
      <link href="css/bootstrap.min.css" rel="stylesheet">
      <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
      <link href="css/theme.css" rel="stylesheet">

      <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&family=Open+Sans:wght@300&display=swap" rel="stylesheet">

      <style>
        /* Footer styling */
        #footer {
           background: #1E3C72; /* Solid blue from the site theme */
           color: white;
           padding: 40px 0; /* More padding for a better layout */
           border-top: 1px solid rgba(255, 255, 255, 0.1); /* Thin top border for separation */
        }
        #footer .container-fluid {
           display: flex;
           justify-content: space-between;
           flex-wrap: wrap; /* Allow wrapping of items to prevent large spaces */
           text-align: center; /* Center text for all sections */
        }
        #footer .span5, #footer .span2 {
           flex: 1;
           margin-bottom: 20px; /* Space between sections */
           padding: 0 20px; /* Add padding to prevent text from going to the sides */
        }
        #footer h3 {
           font-size: 1.4rem;
           margin-bottom: 15px; /* More spacing below headings */
           border-bottom: none; /* Removed border for a cleaner look */
        }
        #footer p, #footer a {
           color: #ddd;
           font-size: 1rem; /* Slightly increased text size */
           line-height: 1.6;
           margin-bottom: 10px; /* More space between lines */
        }
        #footer a:hover {
           text-decoration: underline;
        }
        #footer img {
           max-width: 130px; /* Slightly larger logo */
           height: auto;
           margin-bottom: 20px; /* More space below the logo */
        }
        /* Responsive adjustments for smaller screens */
        @media (max-width: 767px) {
           #footer .container-fluid {
              flex-direction: column;
              text-align: center;
              padding: 0 15px;
           }
           #footer .span5, #footer .span2 {
              width: 100%;
              margin-bottom: 20px;
           }
           #footer h3 {
              font-size: 1.2rem;
           }
           #footer p, #footer a {
              font-size: 0.9rem;
           }
           #footer img {
              max-width: 100px;
           }
        }
        .muted.credit {
           text-align: center;
           font-size: 0.85rem;
           color: #aaa;
           margin-top: 20px;
           border-top: 1px solid rgba(255, 255, 255, 0.1);
           padding-top: 10px;
        }
     </style>

      <style>
         header.jumbotron {
           background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%); /* Gradient background */
           color: white; /* Text color */
           text-align: center; /* Center align text */
           padding: 50px 20px; /* Add some padding for spacing */
           border-radius: 10px; /* Optional: Rounding the corners slightly */
           box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); /* Optional: Adding subtle shadow */
         }
      
         header.jumbotron h1 {
           font-family: 'Montserrat', sans-serif; /* Choose a modern font */
           font-size: 3.5rem; /* Make the title larger */
           font-weight: 700; /* Make the title bold */
           letter-spacing: 2px; /* Increase spacing between letters */
           text-transform: uppercase; /* Make the text all uppercase */
           margin: 0; /* Remove default margin */
         }
      
         header.jumbotron p.lead {
           font-family: 'Open Sans', sans-serif; /* Complementary font for subtitle */
           font-size: 1.5rem; /* Adjust subtitle size */
           font-weight: 300; /* Lighter font weight for a softer look */
           letter-spacing: 1px; /* Slight letter spacing */
           margin-bottom: 20px; /* Add spacing below the subtitle */
         }
      </style>


<style>
   .best-paper {
  font-weight: bold;
  color: #FFD700; /* Gold */
  animation: color-change 2s infinite alternate;
  font-size: 1.2em;
  padding-top: 5px;
}

.honorable-mention {
  font-weight: bold;
  color: #C0C0C0; /* Silver */
  animation: color-change 2s infinite alternate;
  font-size: 1.1em;
  padding-top: 5px;
}

@keyframes color-change {
  0% { color: #FFD700; }
  100% { color: #FF4500; }
}



@media (max-width: 767px) { /* Adjust this breakpoint as needed */
    .nav-collapse.collapse {
        display: none; /* Ensure it's hidden when not toggled */
    }

    .nav-collapse.collapse.in {
        display: block !important; /* Force display block when toggled */
    }

    .navbar .nav-collapse .nav > li {
        display: block; /* Stack the navigation items vertically */
        float: none;
        width: 100%; /* Optional: full width */
    }

    .navbar .nav-collapse .nav > li > a {
        padding: 10px 15px; /* Add padding for tap targets */
        border-bottom: 1px solid #ccc; /* Optional: adds a divider between items */
    }
}


</style>

<style>
    /* Publication content adjustments for full width */
    .publication-row {
       display: flex;
       align-items: flex-start;
       gap: 20px;
       width: 100%; /* Ensure the row spans full width */
       margin-bottom: 40px; /* Space between each publication row */
       padding: 20px 0; /* Add padding for visual spacing */
    }

    .publication-image {
       flex-shrink: 0; /* Prevent image from shrinking */
       width: 800px; /* Set a fixed width for the image */
       height: auto;
       object-fit: contain;
    }

    .publication-details {
       flex-grow: 1; /* Allow details to take up the remaining space */
    }

    .container-fluid {
       width: 100%; /* Make the container use full available width */
       padding-left: 0;
       padding-right: 0;
    }

    section {
       margin-bottom: 40px; /* Space between sections */
    }

    .page-header, .subhead, .publication-row {
       margin-bottom: 20px; /* Keep margin consistent between elements */
    }
 </style>

      
   </head>
   <body>
      <div class="container">
         <header class="jumbotron subhead" id="overview">
            <p class="lead"> User Experience and Interaction Design Lab </p>
            <h1>UXID</h1>
         </header>
         <div class="masthead">
            <div class="navbar">
               <!-- <div class="navbar-inner">
                  <div class="container">
                     <ul class="nav">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="people.html">People</a></li>
                        <li><a href="research.html">Research</a></li>
                        <li class="active"><a href="#">Publications</a></li>
                        <li><a href="award.html">Awards</a></li>
                        <li><a href="ServiceAndtalk.html">Service/Talks</a></li>
                        <li><a href="news.html">News</a></li>
                        <li><a href="teaching.html">Teaching</a></li>
                        <li><a href="contact.html">Contact</a></li>
                     </ul>
                  </div>
               </div> -->

               <div class="navbar-inner">
                  <div class="container">
                      <!-- Button for smaller screens -->
                      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                          <span class="icon-bar"></span>
                          <span class="icon-bar"></span>
                          <span class="icon-bar"></span>
                      </a>
                      <!-- Ensuring that the branding and collapse menu are within the responsive context -->
                      <a class="brand" href="index.html"></a>
                      <div class="nav-collapse collapse">
                          <ul class="nav">
                              <li><a href="index.html">Home</a></li>
                              <li> <a href="people.html">People</a></li>
                              <li><a href="research.html">Research</a></li>
                              <!-- <li><a href="publications.html">Publications</a></li> -->
                              <li class="active"><a href="publications.html">Publications</a></li>
                              <li><a href="award.html">Awards</a></li>
                              <li><a href="ServiceAndtalk.html">Service/Talks</a></li>
                              <li><a href="news.html">News</a></li>
                              <li><a href="teaching.html">Teaching</a></li>
                              <li><a href="contact.html">Contact</a></li>
                          </ul>
                      </div>
                  </div>
              </div>
            </div>
         </div>
         <hr>
         <div class="row-fluid">
            <!-- Full-width publication section -->
            <div>
               <section id="published">
                  <div class="page-header">
                     <h3>Publications</h3>
                  </div>
                     
                     <!-- Publication layout with full-width -->
                     <div class="publication-row">
                        <div>
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:kNdYIx-mwKoC" class="thumbnail">
                              <img src="images/2024_1.png" alt="Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs</h4>
                           <p>Syed Mekael Wasti, Ken Q Pu, Ali Neshati</p>
                           <p>The evolution of Large Language Models (LLMs) has showcased remarkable capacities for logical reasoning and natural language comprehension, enabling solutions that model complex problems. This paper presents a framework serving as an intermediary between users and their interfaces, allowing dynamic interactions.</p>
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:kNdYIx-mwKoC">Springer Nature Switzerland, 2024</a>
                        </div>
                     </div>
                     <!-- Additional 2024 Papers  This study explores expanding earable input using uni-manual gestures in mid-air and on-skin spaces around the ear. Results show optimal performance with up to 3 mid-air and 5 on-skin gesture regions, enhancing input vocabulary for earable devices.-->
                     
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:MXK_kJrjxJIC" class="thumbnail">
                              <img src="images/2024_2.png" alt="Exploring Uni-manual Around Ear Off-Device Gestures for Earables" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Exploring Uni-manual Around Ear Off-Device Gestures for Earables</h4>
                           <p>Shaikh Shawon Arefin Shimon, Ali Neshati, Junwei Sun, Qiang Xu, Jian Zhao</p>
                           <p>Small form factor limits physical input space in earable devices. This study explores
                             off-device inputs using uni-manual gestures in mid-air and on-skin spaces around the ear, 
                             significantly expanding the input vocabulary. Findings show optimal gesture performance with up to 3
                              mid-air and 5 on-skin gesture regions, enhancing interaction potential for earable devices.</p>
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:MXK_kJrjxJIC">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2024</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:KlAtU1dfN6UC" class="thumbnail">
                          <img src="images/MISSINGONE.png" alt="Exploring Unimanual Region-to-Region (R2R) Swipes for Off-Device Earable Interaction" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Exploring Unimanual Region-to-Region (R2R) Swipes for Off-Device Earable Interaction</h4>
                           <p>Shimon Shaikh Shawon, Ali Neshati, Qiang Sun, Junwei Xu, Jian Zhao</p>
                           <p>This study explores how region-to-region swipes in mid-air and on-skin spaces improve earable 
                            interaction by increasing input
                             flexibility and enhancing gesture recognition performance.</p>
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:KlAtU1dfN6UC">IMWUT, 2024</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:Zph67rFs4hoC" class="thumbnail">
                              <img src="images/2024_4.png" alt="Exploring Interactive Color Palettes for Abstraction-Driven Exploratory Image Colorization" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Exploring Interactive Color Palettes for Abstraction-Driven Exploratory Image Colorization</h4>
                           <p>Xinyu Shi, Mingyu Liu, Ziqi Zhou, Ali Neshati, Ryan Rossi, Jian Zhao</p>
                           <p>
                            This study introduces Mondrian, an abstraction-driven approach using interactive color palettes
                             for image colorization. Designed to enhance early ideation in fields like product and graphic design,
                              Mondrian enables humans to work with abstractions while AI handles concrete aspects. Findings show that
                               interactive abstractions promote non-linear exploration and foster creativity, outperforming traditional 
                               tools like Photoshop during the ideation phase.</p>
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:Zph67rFs4hoC">Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, 2024</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:5nxA0vEk-isC" class="thumbnail">
                              <img src="images/2023_1.png" alt="In-vehicle Performance and Distraction for Midair and Touch Directional Gestures" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>In-vehicle Performance and Distraction for Midair and Touch Directional Gestures</h4>
                           <p>Arman Hafizi, Jay Henderson, Ali Neshati, Wei Zhou, Edward Lank, Daniel Vogel</p>
                           <p>This study compares midair swipe gestures and touchscreen swipes for in-vehicle system commands. 
                            Findings reveal that midair gestures are faster and less distracting but less accurate than touchscreen inputs. 
                            Both touchscreen swipes and tapping show similar performance, providing insights for vehicle interface designers 
                            considering gesture-based inputs for center console controls.</p>
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:5nxA0vEk-isC">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 2023</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:0EnyYjriUFMC" class="thumbnail">
                              <img src="images/2023_2.png" alt="De-Stijl: Facilitating Graphics Design with Interactive 2D Color Palette Recommendation" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>De-Stijl: Facilitating Graphics Design with Interactive 2D Color Palette Recommendation</h4>
                           <p>Xinyu Shi, Ziqi Zhou, Jing Wen Zhang, Ali Neshati, Anjul Kumar Tyagi, Ryan Rossi, Shunan Guo, Fan Du, Jian Zhao</p>
                           <p>De-Stijl is an intelligent, interactive color authoring tool designed to assist novice designers in creating
                             harmonious color palettes and fulfilling design constraints. By introducing a novel 2D color
                              palette concept, De-Stijl enhances design efficiency and enables intuitive color perception. 
                              User studies show that it significantly aids in rapid colorization and design iterations compared to existing tools.</p>
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:0EnyYjriUFMC">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 2023</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:hqOjcs7Dif8C" class="thumbnail">
                              <img src="images/2023_3.png" alt="Interaction Region Characteristics for Midair Barehand Targeting on a Television" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Interaction Region Characteristics for Midair Barehand Targeting on a Television</h4>
                           <p>Jay Henderson, Ali Neshati, Sachi Mizobuchi, Wei Zhou, Daniel Vogel, Edward Lank</p>
                           <p>This study investigates the preferred size and position of 2D interaction regions
                             for midair barehand input to control television, focusing on target selection.
                              A field study in participants' homes shows that users can map input space to display space effectively, 
                              revealing consistent and precise hand positioning. The findings offer ideal dimensions for designers developing 
                              barehand input techniques for TV control.</p>
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:hqOjcs7Dif8C">Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems, 2023</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:UebtZRa9Y70C" class="thumbnail">
                              <img src="images/2023_4.png" alt="Slide4n: Creating Presentation Slides from Computational Notebooks with Human-AI Collaboration" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Slide4n: Creating Presentation Slides from Computational Notebooks with Human-AI Collaboration</h4>
                           <p>Fengjie Wang, Xuye Liu, Oujing Liu, Ali Neshati, Tengfei Ma, Min Zhu, Jian Zhao</p>
                           <p>Slide4N is an AI-powered tool designed to help data scientists create presentation slides directly from 
                            computational notebooks. It leverages natural language processing to extract key information from notebook cells
                             and automatically generates slide layouts, offering customization options for further refinement. A user study 
                             showed that participants preferred this human-AI collaboration over fully manual or automatic methods, finding 
                             it useful and efficient for slide creation tasks.</p>
                           <a href="https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XxA76vEAAAAJ&sortby=pubdate&citation_for_view=XxA76vEAAAAJ:UebtZRa9Y70C">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 2023</a>
                        </div>
                     </div>
                     <hr>
                     <div class="publication-row">
                        <div >
                           <a href="https://dl.acm.org/doi/abs/10.1145/3536221.3556586" class="thumbnail">
                              <img src="images/1stPaper.jpg" alt="EdgeSelect: Smartwatch Data Interaction with Minimal Screen Occlusion" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>EdgeSelect: Smartwatch Data Interaction with Minimal Screen Occlusion</h4>
                           <p>A Neshati, A Salo, SA Faleel, Z Li, HN Liang, C Latulipe, P Irani</p>
                           <p>EdgeSelect is an interaction technique designed for smartwatches that addresses common challenges like ‚Äòfat finger‚Äô input and screen occlusion. By focusing on the
                             least obstructed portions of the display, EdgeSelect enables more precise interactions with co-adjacent graphs and other content. 
                             A user study informed the design, leading to a three-layer non-linear interaction method that minimizes occlusion. Further experiments 
                             explored the technique‚Äôs target density limits, showcasing its versatility across different types of content.</p>
                           <a href="https://dl.acm.org/doi/abs/10.1145/3536221.3556586">Proceedings of the 2022 International Conference on Multimodal Interaction, 2022</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://mspace.lib.umanitoba.ca/bitstream/handle/1993/36218/neshati_ali.pdf?sequence=1" class="thumbnail">
                              <img src="images/dataviz.png" alt="Data Visualization and Interaction on Smartwatch Small Screens" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Data Visualization and Interaction on Smartwatch Small Screens</h4>
                           <p>Ali Neshati</p>
                           <p>This thesis presents techniques to optimize data visualization on small smartwatch displays. 
                            It introduces G-Sparks for glanceable line graphs using x-axis compression, and Space-Filling Line Graphs (SF-LG) 
                            to visualize interrelated data efficiently. BezelGlide, a bezel-based interaction technique, reduces screen occlusion when
                             interacting with graphs, improving user experience on smartwatches.</p>
                           <a href="https://mspace.lib.umanitoba.ca/bitstream/handle/1993/36218/neshati_ali.pdf?sequence=1">University of Manitoba, 2022</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://drive.google.com/file/d/1g7LLrJ5alvZimMf4ZXkGyDM2eMQbKIm-/view" class="thumbnail">
                              <img src="images/2ndPaper.png" alt="Understanding and Adapting Bezel-to-Bezel Interactions for Circular Smartwatches" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Understanding and Adapting Bezel-to-Bezel Interactions for Circular Smartwatches</h4>
                           <p>B Rey, K Zhu, ST Perrault, S Bardot, A Neshati, P Irani</p>
                           <p>This study explores bezel-to-bezel (B2B) gestures on circular smartwatches, focusing on their performance in eyes-free, 
                            mobile, and encumbered scenarios. Through two user studies and a machine learning-based analysis, the research aims to improve 
                            B2B gesture accuracy and offers design guidelines for more effective smartwatch control in everyday use.</p>
                           <a href="https://drive.google.com/file/d/1g7LLrJ5alvZimMf4ZXkGyDM2eMQbKIm-/view">Proceedings of the ACM on Human-Computer Interaction, 2022</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="http://hci.cs.umanitoba.ca/assets/publication_files/Crowdsourcing_vs_Laboratory-Style_Social_Acceptability_Studies.pdf" class="thumbnail">
                              <img src="images/21.png" alt="Crowdsourcing vs Laboratory-Style Social Acceptability Studies" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Crowdsourcing vs Laboratory-Style Social Acceptability Studies</h4>
                           <p>Fouad Alallah, Ali Neshati, Nima Sheibani, Yumiko Sakamoto, Andrea Bunt, Pourang Irani, Khalad Hasan</p>
                           <p>This study examines the feasibility of using crowdsourcing platforms for social acceptability research, 
                            particularly for Head-Worn Display (HWD) input modalities. The results show no statistically significant 
                            differences between data collected via crowdsourcing and traditional lab settings, suggesting that crowdsourcing 
                            can be a viable option for conducting social acceptability studies in HCI research.</p>
                           <a href="http://hci.cs.umanitoba.ca/assets/publication_files/Crowdsourcing_vs_Laboratory-Style_Social_Acceptability_Studies.pdf">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 2021</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://drive.google.com/file/d/117OI1AtFsyI3jGRwpejPFFL8udHoZAqi/view" class="thumbnail">
                              <img src="images/3rdPaper.png" alt="BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion</h4>
                           <p>A Neshati, B Rey, AS Mohommed Faleel, S Bardot, C Latulipe, P Irani</p>
                           <p>This paper introduces BezelGlide, a set of bezel interaction techniques designed to minimize screen occlusion 
                            and "fat finger" issues on smartwatches. Two user studies were conducted, revealing that Partial BezelGlide (PBG) 
                            outperforms Full BezelGlide (FBG) and other techniques in accuracy and user preference for graph interaction, especially 
                            in mobile scenarios. PBG shows potential for general use across various graph types on smartwatches.</p>
                           <a href="https://drive.google.com/file/d/117OI1AtFsyI3jGRwpejPFFL8udHoZAqi/view">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 2021</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://drive.google.com/file/d/1y_aMi8Tid_SBBSeDqDNK3o33NBMWLCig/view" class="thumbnail">
                              <img src="images/4thpaper.png" alt="SF-LG: Space-Filling Line Graphs for Visualizing Interrelated Time-series Data on Smartwatches" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>SF-LG: Space-Filling Line Graphs for Visualizing Interrelated Time-series Data on Smartwatches</h4>
                           <p>A Neshati, F Alallah, B Rey, Y Sakamoto, M Serrano, P Irani</p>
                           <p>This paper introduces the Space-Filling Line Graph (SF-LG), a technique designed to visualize
                             multiple interrelated time-series datasets on small smartwatch displays. SF-LG optimizes screen space 
                             while maintaining the key properties of time-series graphs. User studies show that SF-LG enables quick and 
                             accurate data comprehension and supports efficient linking of related content, offering guidelines for maximizing 
                             display space on smartwatches.</p>
                           <a href="https://drive.google.com/file/d/1y_aMi8Tid_SBBSeDqDNK3o33NBMWLCig/view">Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction, 2021</a>
                           <div class="honorable-mention">üèÖ Honorable Mention Award</div>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://dl.acm.org/doi/10.1145/3396339.3396394" class="thumbnail">
                              <img src="images/5thPpaer.png" alt="Using Guessability Framework: Age-related Differences in Hand Gesture Interaction" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Using Guessability Framework: Age-related Differences in Hand Gesture Interaction</h4>
                           <p>Y Vasylkiv, A Neshati, SAM Faleel, Y Sakamoto, P Irani</p>
                           <p>This study examines age-related differences in hand gesture preferences between younger and older adults in the context 
                            of mid-air gestures. It highlights the importance of age-inclusive design to ensure that technologies are accessible and 
                            adoptable by older adults, given their diverse motor abilities. This research contributes to creating more inclusive and 
                            effective gesture-based interaction modalities for all age groups.</p>
                           <a href="https://dl.acm.org/doi/10.1145/3396339.3396394">Proceedings of the 11th Augmented Human International Conference, 2020</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://hci.cs.umanitoba.ca/assets/publication_files/Sandra_paper.pdf" class="thumbnail">
                              <img src="images/6thPaper.png" alt="Eyes-Free Graph Legibility: Using Skin-Dragging to Provide a Tactile Graph Visualization on the Arm" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Eyes-Free Graph Legibility: Using Skin-Dragging to Provide a Tactile Graph Visualization on the Arm</h4>
                           <p>S Bardot, S Rempel, B Rey, A Neshati, Y Sakamoto, C Menon, P Irani</p>
                           <p>This study explores the use of tactile displays to convey data from charts, such as line graphs, through skin-dragging techniques. 
                            A prototype was developed to test the feasibility of conveying data in eyes-free scenarios, common in on-the-go computing. The results 
                            from an experiment (n=12) show that participants performed equally well with two skin-dragging methods, Full-Drag and Dot, but Full-Drag 
                            was significantly preferred. The study concludes with design guidelines for tactile displays focusing on graph representations.</p>
                           <a href="https://hci.cs.umanitoba.ca/assets/publication_files/Sandra_paper.pdf">Proceedings of the 11th Augmented Human International Conference, 2020</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://hal.science/hal-02891743/file/neshati_26190.pdf" class="thumbnail">
                              <img src="images/7thPaper.png" alt="G-Sparks: Glanceable Sparklines on Smartwatches" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>G-Sparks: Glanceable Sparklines on Smartwatches</h4>
                           <p>A Neshati, Y Sakamoto, L Leboe-Mcgowan, J Leboe-McGowan, M Serrano, P Irani</p>
                           <p>This research introduces G-Sparks, a compact visual representation of glanceable line graphs for smartwatches. 
                            The study examines optimal line-graph compression approaches, finding that compressing along the x-axis improves
                             height estimation accuracy without compromising perceptual metrics. The research also shows that even at high compression 
                             levels, legibility remains intact for glanceable tasks. These findings offer valuable guidelines for integrating line charts 
                             into smartwatch applications, minimizing interaction effort while maintaining data clarity.</p>
                           <a href="https://hal.science/hal-02891743/file/neshati_26190.pdf">45th Conference on Graphics Interface (GI 2019), 2019</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://hal.science/hal-03030544/file/MODEL-CHI19.pdf" class="thumbnail">
                              <img src="images/8thPaper.png" alt="An Analytic Model for Time-Efficient Personal Hierarchies" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>An Analytic Model for Time-Efficient Personal Hierarchies</h4>
                           <p>W Delamare, A Neshati, P Irani, X Ren</p>
                           <p>This study introduces an analytic model that helps optimize modifiable hierarchy structures, such as file systems or 
                            smartphone app launchers, by recommending local modifications like folder creation. This approach allows end-users flexibility 
                            in organizing their content while benefiting from faster item retrieval. Experiments show that the model's recommendations improve 
                            selection times and are applicable across various hierarchy layouts, including linear, radial, and grid structures.</p>
                           <a href="https://hal.science/hal-03030544/file/MODEL-CHI19.pdf">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, 2019</a>
                           <div class="honorable-mention">üèÖ Honorable Mention Award</div>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://ebooks.iospress.nl/volumearticle/51200" class="thumbnail">
                              <img src="images/9thpaper.png" alt="Smart Home Interactions for People with Reduced Hand Mobility Using Subtle EMG-Signal Gestures" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Smart Home Interactions for People with Reduced Hand Mobility Using Subtle EMG-Signal Gestures</h4>
                           <p>Y Vasylkiv, A Neshati, Y Sakamoto, R Gomez, K Nakamura, P Irani</p>
                           <p>This paper explores the use of electromyography (EMG) sensors to enhance smart home control for individuals with limited hand mobility.
                             By capturing subtle muscle activity, EMG enables gesture-based interactions that would be difficult to sense otherwise, offering a 
                             promising solution for users with reduced upper limb motion. The study highlights the potential of EMG technologies in improving 
                             accessibility and shaping smart environments for users who require assistance in their daily routines.</p>
                           <a href="https://ebooks.iospress.nl/volumearticle/51200">Improving Usability, Safety and Patient Outcomes with Health Information Technology, 2019</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://ebooks.iospress.nl/doi/10.3233/978-1-61499-951-5-325" class="thumbnail">
                              <img src="images/10thPaper.png" alt="Challenges in Displaying Health Data on Small Smartwatch Screens" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Challenges in Displaying Health Data on Small Smartwatch Screens</h4>
                           <p>A Neshati, Y Sakamoto, P Irani</p>
                           <p>This paper addresses the challenges of visualizing health-related data directly on smartwatches, which are increasingly used for self-tracking due to their embedded sensors. 
                            With form factor limitations hindering data access and interpretation, users often rely on secondary devices like smartphones.
                             The research focuses on identifying and implementing novel visualization techniques to improve user interaction with health data 
                             directly on the smartwatch. Key challenges and methods for overcoming them are presented.</p>
                           <a href="https://ebooks.iospress.nl/doi/10.3233/978-1-61499-951-5-325">Improving Usability, Safety and Patient Outcomes with Health Information Technology, 2019</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://hci.cs.umanitoba.ca/assets/publication_files/PerformVSObserve.pdf" class="thumbnail">
                              <img src="images/11thpaper.png" alt="Performer vs Observer: Whose Comfort Level Should We Consider When Examining the Social Acceptability of Input Modalities for Head-Worn Displays?" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Performer vs Observer: Whose Comfort Level Should We Consider When Examining the Social Acceptability of Input Modalities for Head-Worn Displays?</h4>
                           <p>F Alallah, A Neshati, Y Sakamoto, K Hasan, E Lank, A Bunt, P Irani</p>
                           <p>This paper explores the social acceptability of interactions with head-worn displays (HWDs) like VR and AR headsets, focusing on both the performer (user) and observer (spectator) perspectives. 
                            While most studies examine social acceptability from the user's viewpoint, this research contrasts both perspectives in various social 
                            contexts. Results highlight key similarities and differences, emphasizing the need to consider observers' viewpoints. The paper offers 
                            guidelines for understanding social acceptability from the observer's perspective, complementing current practices in evaluating 
                            interactions with emerging technologies.</p>
                           <a href="https://hci.cs.umanitoba.ca/assets/publication_files/PerformVSObserve.pdf">Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology, 2018</a>
                        </div>
                     </div>
                     <hr>
                     
                     <div class="publication-row">
                        <div >
                           <a href="https://www.researchgate.net/profile/Mehdi-Ghasemi-18/publication/295504931_Is_Really_NACK_Protocol_Secure_to_Be_Employed_in_MANETs/links/619e18e44fe4ea1cea97e7cd/Is-Really-NACK-Protocol-Secure-to-Be-Employed-in-MANETs.pdf" class="thumbnail">
                              <img src="images/13thPaper.png" alt="Is Really NACK Protocol Secure to Be Employed in MANETs?" class="publication-image">
                           </a>
                        </div>
                        <div class="publication-details">
                           <h4>Is Really NACK Protocol Secure to Be Employed in MANETs?</h4>
                           <p>M Saeed, A Mackvandi, M Taghavi, MZ Bidoki, M Ghasemi, A Neshati</p>
                           <p>This paper analyzes the NACK protocol, originally proposed for Mobile Ad-hoc Networks (MANETs), which are decentralized networks of self-organizing wireless nodes. 
                            MANETs are widely used in scenarios such as disaster relief and military operations, making security a critical concern. 
                            The paper focuses on the vulnerabilities of the NACK protocol, specifically its susceptibility to man-in-the-middle attacks, 
                            a form of routing misbehavior. The study identifies technical flaws in the protocol and proposes countermeasures to address these 
                            security risks.</p>
                           <a href="https://www.researchgate.net/profile/Mehdi-Ghasemi-18/publication/295504931_Is_Really_NACK_Protocol_Secure_to_Be_Employed_in_MANETs/links/619e18e44fe4ea1cea97e7cd/Is-Really-NACK-Protocol-Secure-to-Be-Employed-in-MANETs.pdf">2014 IEEE 17th International Conference on Computational Science and Engineering, 2014</a>
                        </div>
                     </div>
                     <hr>
                                                                                                         
                  <div class="page-header">
                     <h3>In Review and Preperation</h3>
                  </div>
                  <ol>
                     <li class="publication">Ali Neshati, Ehsan Jahangirzadeh, Hanyu Xu, Jian Zhao, ‚ÄúIVA: An Empirical Investigation and Design of Intelligent Virtual Assistant for Online Meetings‚Äù, 41st International Conference on Human Factors in Computing Systems (CHI 2025)</li>
                     <!-- <li class="publication">Arman Hafizi, Ali Neshati, Jay Henderson, Wei Zhou, Edward Lank, Daniel Vogel, ‚ÄúIn-Vehicle Performance and Distraction for Midair and Touch Directional Gestures", 41st International Conference on Human Factors in Computing Systems (CHI 2023)</li> -->
                     <!-- <li class="publication">Jay Henderson, Ali Neshati, Sachi Mizobuchi, Wei Zhou, Daniel Vogel, Edward Lank, ‚ÄúInteraction Region Characteristics for Midair Barehand Targeting on a Television‚Äù, 41st International Conference on Human Factors in Computing Systems (CHI 2023)</li> -->
                     <!-- <li class="publication">Xinyu Shi, Ziqi Zhou, Jiang Zhang, Ali Neshati, Anjul Tyagi, Ryan Rossi, Shunan Guo, Fan Du, Jian Zhao, ‚ÄúDe-Stijl: Facilitating Graphics Design with Interactive 2D Color Palette Recommendation‚Äù, 41st International Conference on Human Factors in Computing Systems (CHI 2023)</li> -->
                     <!-- <li class="publication">Fengjie Wang, Xuye Liu, Oujing Liu, Ali Neshati, Tengfei Ma, Min Zhu, Jian Zhao, ‚ÄúSlide4N: Creating Presentation Slides from Computational Notebooks with Human-AI Collaboration‚Äù, 41st International Conference on Human Factors in Computing Systems (CHI 2023)</li> -->
                  </ol>
               <hr>

            </div>
         </div>
      </div>



      <footer id="footer">
        <div class="container-fluid">
           <div class="row-fluid">
              <div class="span5">
                 <h3>Contact Information</h3>
                 <p><b>Office Hours:</b> Monday-Friday (8:00am - 5:00pm)</p>
                 <p><b>Phone:</b> ext. 5305</p>
                 <a href="mailto:ali.neshati@ontariotechu.ca">Email: ali.neshati@ontariotechu.ca</a>
              </div>
              <div class="span2">
                 <a href="https://mybrandnewlogo.com/"><img src="images/logo.png" alt="research-lab-logo"/></a>
              </div>
              <div class="span5">
                 <h3>Address</h3>
                 <p>Ontario Tech University<br>
                    Oshawa, Ontario, Canada<br>
                    Office: UA building - Room UA 4041<br>
                    UXID Lab: UA building - Room UA 2071</p>
                 <a href="https://maps.app.goo.gl/BbkBNisWc82sD63B8/" target="_blank">Show Map</a>
              </div>
           </div>
        </div>
        <div class="container">
           <p class="muted credit">¬© 2024 UXID Research Lab, Ontario Tech University.</p>
        </div>
     </footer>


      <!-- <footer id="footer">
         <div class="container-fluid">
            <div class="row-fluid">
               <div class="span5">
                  <h3>Contact Information</h3>
                  <p><b>Office Hours: </b>Monday-Friday (8.00am - 5.00pm)</p>
                  <p><b>ext.: </b>5305</p>
                  <a href="mailto:ali.neshati@ontariotechu.ca">Email: ali.neshati@ontariotechu.ca</a>
               </div>
               <div class="span2">
                  <a href="https://mybrandnewlogo.com/"><img src="images/logo.png" alt="research-lab-logo"/></a>
               </div>
               <div class="span5">
                  <h3>Address</h3>
                  <p>Ontario Tech University<br>
                     Oshawa, Ontario, Canada<br>
                     Office: UA building - Room UA 4041<br>
                     UXID Lab: UA building - Room UA 2071
                  </p>
                  </p>
                  <a href="https://maps.app.goo.gl/BbkBNisWc82sD63B8/" target="_blank">Show Map</a>
               </div>
            </div>
         </div>
         <div class="container">
            <p class="muted credit">¬© 2024 UXID Research Lab, Ontario Tech University.</p>
         </div>
      </footer> -->

      <!-- Le javascript
         ================================================== -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="js/jquery-1.9.1.min.js"></script>
 
      <script>
         $(document).ready(function() {
             $(document.body).scrollspy({
                 target: "#navparent"
             });
         });
         
      </script>
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
      <script>
          $(document).ready(function() {
              $('.btn-navbar').click(function() {
                  $('.nav-collapse').collapse('toggle');
              });
          });
      </script>
      
   </body>
</html>
